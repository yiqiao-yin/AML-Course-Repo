{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ST5242_HW4_Yiqiao_Yin_YY2502.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"ViqUvwocbvkv","colab_type":"text"},"cell_type":"markdown","source":["# Homework 4: Transfer learning with the inception network\n","\n","This is Homework #4. My name is Yiqiao Yin and my UNI is yy2502. I approve this message.\n","\n","## DUE Friday 17 November at 4pm (both sections)\n","Submit your homework by publishing a notebook that cleanly displays your code, results, and plots to pdf or html\n","\n","## Description\n","\n","In this homework, we will fine-tune the existing [inception network](https://arxiv.org/pdf/1512.00567.pdf) to classify some new categories of images. Building and fitting the network from scratch is expensive and far beyond the scope of this assignment, so you will load the tensorflow graph and variables which have been pre-trained on the imagenet dataset. You are responsible for completing the skeleton code in this notebook in order to use tensorboard to investigate the graph architecture, gain insight into the behavior of the pre-trained network on a new image classification task, fine-tune the network for this new task (retraining the last layer), and finally evaluate the performance of the fine-tuned model.\n","\n","## Downloading the network and data\n","\n","In order to avoid clutter and extraneous technical details, we have created a supplementary module  `transfer_learning.py` that does most of the heavy lifting. We have only exposed the high-level which you are responsible for completing and understanding in full in this notebook.  While you are encouraged to read `transfer_learning.py`, you should not need to modify it in any way.  To import this module into colab, you must first execute the cell below, and then second you must upload the transfer_learning.py file using the upload button that will appear."]},{"metadata":{"id":"ER3YaFuGiim1","colab_type":"code","outputId":"2b733c77-68e6-4f42-f1d1-7653695c669c","executionInfo":{"status":"ok","timestamp":1542473045867,"user_tz":300,"elapsed":7699,"user":{"displayName":"Yiqiao Yin","photoUrl":"https://lh5.googleusercontent.com/-RMhKrlmmtRk/AAAAAAAAAAI/AAAAAAAAAMo/Lr-4QKINZzM/s64/photo.jpg","userId":"04761967746966690780"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["# Import transfer_learning module. \n","from google.colab import files\n","src = list(files.upload().values())[0]\n","open('transfer_learning.py','wb').write(src)\n","import transfer_learning"],"execution_count":84,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-1735ab06-7ecb-45b9-991d-94852fc4ab0c\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-1735ab06-7ecb-45b9-991d-94852fc4ab0c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving transfer_learning.py to transfer_learning (9).py\n"],"name":"stdout"}]},{"metadata":{"id":"i1SC0xW4bvkx","colab_type":"code","colab":{}},"cell_type":"code","source":["# Import the rest\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import os.path"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hYE1EeBHbvk3","colab_type":"text"},"cell_type":"markdown","source":["We will be using tensorboard to visualize the network, inputs, and other summaries of the training process. After running the following cell, follow the instructions for using tensorboard, evailable in Andrew's excellent tensorflow tutorial."]},{"metadata":{"id":"PuLHDhu7bvk4","colab_type":"code","colab":{}},"cell_type":"code","source":["# Ensure target log dir exists\n","INCEPTION_LOG_DIR = './tmp/inception_v3_log'\n","if not os.path.exists(INCEPTION_LOG_DIR):\n","    os.makedirs(INCEPTION_LOG_DIR)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"USQsTycQbvk7","colab_type":"text"},"cell_type":"markdown","source":["Your goal in the assignment is to use a small collection of photos (500 to be exact) to construct a flower-image classifier capable of discerning differences between daisies, roses, dandelions, sunflowers, and tulips. This presents a obstacle to the use of state-of-the-art image classification models due to the heavily underdetermined nature of the problem (# features = #pixels * #channels >> #images). Moreover, training a large convolutional neural network from the ground up is can be computationally prohibitive. Alternatively, strongly biased linear classifiers like regularized logistic regression avoid these pitfalls. However, the performance of these models will be inadequate, because most important image features are non-linear transformations of the raw input and it is not obvious what these non-linear transformations should be.\n","\n","To circumvent these challenges, we will leverage what a pre-trained convolutional neural network has already learned about important image features from the [imagenet dataset](http://www.image-net.org/) by fine tuning it to our specific problem. To begin, download the the flower dataset [here](http://download.tensorflow.org/example_images/flower_photos.tgz) and extract the contents into a sub-folder of your working directory named ```data```. This dataset is from a [Google Code Lab](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0), though note that we ask you to do considerably more than in that lab, so it is not helpful (nor are the commands covered therein valid answers to this homework).\n","\n","The following cell will partition the images into a training and testing set, then it will load lists of image paths for each."]},{"metadata":{"id":"-gHrqpAFjwuB","colab_type":"code","colab":{}},"cell_type":"code","source":["# download and extract flower data\n","transfer_learning._maybe_download_and_extract('http://download.tensorflow.org/example_images/flower_photos.tgz')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PdbzWbc5bvk8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"b974f7f4-d1dc-4497-92ed-bcc6513ffa16","executionInfo":{"status":"ok","timestamp":1542473071619,"user_tz":300,"elapsed":740,"user":{"displayName":"Yiqiao Yin","photoUrl":"https://lh5.googleusercontent.com/-RMhKrlmmtRk/AAAAAAAAAAI/AAAAAAAAAMo/Lr-4QKINZzM/s64/photo.jpg","userId":"04761967746966690780"}}},"cell_type":"code","source":["# For performance reasons, we only use 100 images per class.\n","training_images, testing_images, label_maps = transfer_learning.create_image_lists(\n","    './model/flower_photos', testing_percentage=10, max_number_images=100)"],"execution_count":88,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Looking for images in 'sunflowers'\n","WARNING:tensorflow:WARNING: Folder sunflowers has more than 100 images. Some images will never be selected.\n","INFO:tensorflow:Looking for images in 'dandelion'\n","WARNING:tensorflow:WARNING: Folder dandelion has more than 100 images. Some images will never be selected.\n","INFO:tensorflow:Looking for images in 'daisy'\n","WARNING:tensorflow:WARNING: Folder daisy has more than 100 images. Some images will never be selected.\n","INFO:tensorflow:Looking for images in 'roses'\n","WARNING:tensorflow:WARNING: Folder roses has more than 100 images. Some images will never be selected.\n","INFO:tensorflow:Looking for images in 'tulips'\n","WARNING:tensorflow:WARNING: Folder tulips has more than 100 images. Some images will never be selected.\n"],"name":"stdout"}]},{"metadata":{"id":"3IPqEV60bvk_","colab_type":"text"},"cell_type":"markdown","source":["Next, we will load the pre-trained inception model and look at its architecture in tensorboard. \n","\n","#### 1). Complete the following code block by opening a filewriter and pass it the graph object we have created."]},{"metadata":{"id":"bidVvSLXbvk_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create the inception model.\n","# Warning! The first time you run this, it will try to download the inception model\n","# from the internet. This is somewhat large (80MB), and can sometimes fail. If\n","# it does, simply try again.\n","\n","graph, bottleneck, resized_input, softmax = transfer_learning.create_model()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9tRLMpXGbvlD","colab_type":"code","colab":{}},"cell_type":"code","source":["# Use a summary writer to write the loaded graph and display it in tensorboard.\n","# Then answer the questions below\n","with graph.as_default():\n","    jpeg_data, decoded_image = transfer_learning.make_jpeg_decoding()\n","    #-- Students Complete\n","    sess = tf.Session()\n","    writer = tf.summary.FileWriter(INCEPTION_LOG_DIR, sess.graph)\n","    #--"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6Wtqd5UXjXYt","colab_type":"code","colab":{}},"cell_type":"code","source":["# Run Tensorflow in the background - note that we specify the log \n","# directory we want to look at\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(INCEPTION_LOG_DIR)\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"14EAR9m8jZEz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"outputId":"90b64daa-ab79-497a-fb3e-b3898e3d1f22","executionInfo":{"status":"ok","timestamp":1542472974231,"user_tz":300,"elapsed":10634,"user":{"displayName":"Yiqiao Yin","photoUrl":"https://lh5.googleusercontent.com/-RMhKrlmmtRk/AAAAAAAAAAI/AAAAAAAAAMo/Lr-4QKINZzM/s64/photo.jpg","userId":"04761967746966690780"}}},"cell_type":"code","source":["# Download and unzip ngrok - you will only need to do this once per session\n","! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip ngrok-stable-linux-amd64.zip"],"execution_count":81,"outputs":[{"output_type":"stream","text":["--2018-11-17 16:42:40--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 35.173.6.94, 34.196.237.103, 52.204.188.97, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|35.173.6.94|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5363700 (5.1M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n","\n","\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \rngrok-stable-linux- 100%[===================>]   5.11M  --.-KB/s    in 0.1s    \n","\n","2018-11-17 16:42:40 (42.4 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [5363700/5363700]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"metadata":{"id":"jnb5fOFrjd67","colab_type":"code","colab":{}},"cell_type":"code","source":["# Launch the ngrok background process\n","get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T4v4j7ldjfg1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8e7149c2-3637-42d4-ed4f-106a96ef7d7e","executionInfo":{"status":"ok","timestamp":1542473107240,"user_tz":300,"elapsed":1288,"user":{"displayName":"Yiqiao Yin","photoUrl":"https://lh5.googleusercontent.com/-RMhKrlmmtRk/AAAAAAAAAAI/AAAAAAAAAMo/Lr-4QKINZzM/s64/photo.jpg","userId":"04761967746966690780"}}},"cell_type":"code","source":["# Get the public URL and be sorted!\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":93,"outputs":[{"output_type":"stream","text":["https://e1cd9ec8.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"fgsdzYExbvlH","colab_type":"text"},"cell_type":"markdown","source":["#### 2.) In tensorboard, expand the 'softmax' and then the 'logits' block in order to answer to following questions:\n","* What are the dimensions of the inputs/outputs to/from the softmax block?\n","* What are the shape and dimension of the input images?\n","* When considered in isolation, what model is the 'softmax' block implementing? Explain."]},{"metadata":{"id":"kAdnqhdjdGMu","colab_type":"code","colab":{}},"cell_type":"code","source":["# Download and unzip ngrok - you will only need to do this once per session\n","! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip ngrok-stable-linux-amd64.zip\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f0WsTVuOdTK5","colab_type":"code","colab":{}},"cell_type":"code","source":["# Launch the ngrok background process\n","get_ipython().system_raw('./ngrok http 6006 &')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6QYpjqQVdXF8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"466db83e-ee72-4a72-9049-98e355c9eccd","executionInfo":{"status":"ok","timestamp":1542472649565,"user_tz":300,"elapsed":1092,"user":{"displayName":"Yiqiao Yin","photoUrl":"https://lh5.googleusercontent.com/-RMhKrlmmtRk/AAAAAAAAAAI/AAAAAAAAAMo/Lr-4QKINZzM/s64/photo.jpg","userId":"04761967746966690780"}}},"cell_type":"code","source":["# Get the public URL and be sorted!\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n"],"execution_count":77,"outputs":[{"output_type":"stream","text":["http://e1cd9ec8.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"dZgFcPPhbvlI","colab_type":"text"},"cell_type":"markdown","source":["Support your answers by including screenshots from tensorboard. (to insert images, simply copy and paste into the notebook)\n","\n"]},{"metadata":{"id":"REbCNEuabvlJ","colab_type":"text"},"cell_type":"markdown","source":["Write your answers here.\n","\n","- Dimension of inputs: 1x2048;\n","- Dimension of output: 1x1008.\n","- The shape and dimension of the inpute images are 299x299x3.\n","- We can use logistic regression."]},{"metadata":{"id":"e7UOl-tIbvlL","colab_type":"text"},"cell_type":"markdown","source":["## Classifying out of the box\n","\n","Before retraining or modifying the network, we would like to better understand how it behaves on our dataset out of the box. Therefore, we will use it classify a couple of our flower images. First, we must create summary operations to write quanities of interest to tensorboard so that we may investigate. \n","\n","#### 3.) Complete the block below by implementing the following summaries:\n","* `output_summary`: a summary of output probabilities (histogram of the output of the softmax block)\n","* `input_summary`: a summary of image input to the network (image of resized and decoded jpeg data)\n","* `bottleneck_summary`: a summary of bottleneck activations (histogram of the input to the softmax block)"]},{"metadata":{"id":"MafS0VfabvlM","colab_type":"code","colab":{}},"cell_type":"code","source":["with graph.as_default():\n","    # Define summaries for tensorboard\n","    #-- Students complete\n","    # histogram of softmax output\n","    \n","    # display image of input\n","    \n","    # second to last layer output\n","    with tf.Session() as sess:\n","        output_summary = tf.summary.histogram(\"output_summary\", softmax)\n","        input_summary = tf.summary.image(\"input_summary\", resized_input)\n","        bottleneck_summary = tf.summary.histogram(\"bottleneck_summary\", bottleneck)\n","    #-- \n","    summary_op = tf.summary.merge_all()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"co28sR9mbvlT","colab_type":"text"},"cell_type":"markdown","source":["Next, we will need a function that streamlines the process of performing inference given an image. This function will take as input the tensorflow session, a path to the image, and the ```summary_op``` we have created above. It will then open a ```t.summary.FileWriter``` and run the network forwards to obtain the formatted image, softmax output, and summaries. \n","\n","#### 4). Complete the code block below as instructed in each of the comments."]},{"metadata":{"id":"cRHlywDgbvlV","colab_type":"code","colab":{}},"cell_type":"code","source":["def classify_image(session, image_path, summary_op):\n","    \"\"\"This functions reads a single image from disk and classifies\n","    the image using the pre-trained network.\n","    \n","    Parameters\n","    ----------\n","    session: the tensorflow session to use to run the operation\n","    image_path: a string corresponding to the path of the image to load\n","    summary_op: the summary operation.\n","    \n","    Returns\n","    -------\n","    label: an integer representing the label of the classified example.\n","    softmax_output: the network's output multinomial probabilities\n","    \"\"\"\n","    \n","    # Open single image file and extract data\n","    with open(image_path, 'rb') as f:\n","        image_data = f.read()\n","    \n","    #-- Students Complete\n","    # Create Summary writer object\n","    writer = tf.summary.FileWriter(INCEPTION_LOG_DIR)\n","    \n","        \n","    # Run pre-defined pipeline to decode & resize raw image data into decoded_image \n","    resized_input_values = session.run(decoded_image,\n","                                  {jpeg_data: image_data})\n","    \n","        \n","    # Run input through network and obtain output from last layer, defined by create_model() call above.\n","    # Note: be sure to run your summary operation as well.\n","    bottleneck_values = session.run(bottleneck,\n","                               {resized_input: resized_input_values})\n","    softmax_output = session.run(softmax,\n","                               {bottleneck: bottleneck_values})\n","    summary_out = session.run(summary_op, {resized_input: resized_input_values, bottleneck: bottleneck_values, softmax:softmax_output})\n","    \n","    \n","    # Log summary & terminate writer in the usual way.\n","    writer.add_summary(summary_out)\n","    writer.close()\n","   \n","  \n","    #--\n","    \n","    # Return label\n","    return(np.argmax(softmax_output),softmax_output)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4ydvij6BbvlY","colab_type":"text"},"cell_type":"markdown","source":["We are now ready to classify some images. Run the code in this cell in order to obtain the output label and navigate to tensorboard to look at the histogram and image summaries that have been logged. **Note:** You may need to refresh Tensorboard in order for this to display."]},{"metadata":{"id":"O1jHmDjhbvlZ","colab_type":"code","colab":{}},"cell_type":"code","source":["image_path = os.path.join(os.getcwd(), 'model', 'flower_photos', 'daisy', '100080576_f52e8ee070_n.jpg')\n","with graph.as_default():\n","    with tf.Session() as session:\n","        # We classify the image and print the label\n","        imagenet_label,_ = classify_image(session, image_path, summary_op)\n","print(imagenet_label)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yDqWI37xbvlc","colab_type":"text"},"cell_type":"markdown","source":["#### 5.) Confirm that your code works by taking a screenshot of the input image in tensorboard."]},{"metadata":{"id":"jmVSM3IFbvld","colab_type":"text"},"cell_type":"markdown","source":["Include your screenshot here."]},{"metadata":{"id":"9cxUhc-_bvld","colab_type":"text"},"cell_type":"markdown","source":["If you've completed the prevous steps correctly, you should see that the pre-trained network assigns the label 357 to our image. Cross referencing this with a [source](https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57) that explains what each of the imagenet labels are, we see that our classifier is right on target. The image was indeed a daisy, but this is solely a consequence of the fact that the class 'daisy' is included in the imagenet dataset. We should not expect such favorable outcome when this is not the case. We will test this assumtion by classifying a different type of flower image."]},{"metadata":{"id":"7aaVgzzjbvle","colab_type":"code","colab":{}},"cell_type":"code","source":["image_path = os.path.join(os.getcwd(), 'model', 'flower_photos', 'roses', '19440805164_920b28da61_n.jpg')\n","with graph.as_default():\n","    with tf.Session() as session:\n","        imagenet_label,softmax_output = classify_image(session, image_path, summary_op)\n","print(imagenet_label)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"56AgWWHnYn8y","colab_type":"code","colab":{}},"cell_type":"code","source":["max(softmax_output[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"66sFKYUcbvlg","colab_type":"text"},"cell_type":"markdown","source":["#### 6.) Please explain what you have found by answering the following questions:\n","\n","* What does the network think this is an image of? Is this reasonable?\n","* How confident is it in assigning this label (what is the probability it assigns)?\n","* Why might the network behave this way?\n","* What warning might you give about deep-learning/machine-learning in general based on these observations?\n","\n","Support your answers by referencing screenshots from tensorboard (of the input picture, softmax output, etc.)."]},{"metadata":{"id":"fLNr9imUbvlj","colab_type":"text"},"cell_type":"markdown","source":["Answer these questions here.\n","\n","- Dimension of inputs: 1x2048;\n","- Dimension of output: 1x1008.\n","- The shape and dimension of the inpute images are 299x299x3.\n","- We can use logistic regression."]},{"metadata":{"id":"-txuVP3Abvlk","colab_type":"text"},"cell_type":"markdown","source":["## Adapting The Network\n","\n","Clearly, this model will not suffice for our application out of the box. However, it is not unreasonable to think that we may be able to adapt what this network has \"learned\" about processing images for our purposes. Remember that the last layer of the network implements a well known classifier (see your answer to question 2). Under this interpretation, the layers of the network up until the last are responsible for learning a non-linear mapping between raw inputs (pixel values) and useful derived features. Consequently, we may remove the last layer, fix the weights of the remaining layers, use what remains as a black-box function transforming images into derived feature vectors, and finally fit a new classifier on the derived feature vectors. "]},{"metadata":{"id":"OAsNpY_Nbvll","colab_type":"text"},"cell_type":"markdown","source":["### Pre-Computing the bottlenecks\n","\n","To train the new classifier efficiently, we will want to pre-compute the derived feature vectors. We have been calling these the 'bottleneck', because they would limit the speed of training if we had to be re-computed them iteration. Additionally, its small size relative to the dimension of our raw input is essential in ensures that it provides a compact summary of the images for our classifier.\n","\n","#### 7). Which block in the tensorboard graph is the bottleneck?"]},{"metadata":{"id":"-Co-3jD6bvln","colab_type":"text"},"cell_type":"markdown","source":["Write you answer here.\n","\n","-The bottleneck is the pool_3 block."]},{"metadata":{"id":"W5KACbaabvln","colab_type":"text"},"cell_type":"markdown","source":["We have written code that will cache the derived features or 'bottleneck activation' for each images. In order to use this, we require that you complete a simple function that returns these activations.\n","\n","#### 8). Complete the code block below and compute the bottlenecks."]},{"metadata":{"id":"WXSuTYZObvlp","colab_type":"code","colab":{}},"cell_type":"code","source":["def compute_bottleneck(session, image_data):\n","    \"\"\"Computes the bottleneck for a given image\n","    \n","    Parameters\n","    ----------\n","    session: the tensorflow session to use for the computation.\n","    jpeg_data_tensor: the tensor to feed the jpeg data into.\n","    bottleneck_tensor: the tensor representing.\n","    image_data: a byte sequence representing the encoded jpeg data.\n","    \n","    Returns\n","    -------\n","    A numpy array containing the bottleneck information for the image.\n","    \"\"\"\n","    #-- To be completed by the student\n","    num = session.run(decoded_image, feed_dict={jpeg_data: image_data})\n","    bottleneck_output = session.run((bottleneck), feed_dict={resized_input: num})\n","    return(bottleneck_output) \n","    \n","    #--"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O0om-VUGbvlr","colab_type":"code","colab":{}},"cell_type":"code","source":["# This cell generates all the bottlenecks. Warning: it may take a while\n","# The results are cached so you only need to do it once -- if you change\n","# your compute_bottleneck function, you will need to delete the existing\n","# files to force the notebook to regenerate the bottlenecks (they are\n","# found in ./data/bottlenecks)\n","\n","with graph.as_default():\n","    with tf.Session() as session:\n","        transfer_learning.cache_bottlenecks(compute_bottleneck, session, training_images)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nE-IaGa6bvlu","colab_type":"text"},"cell_type":"markdown","source":["Now that these have been saved, we can load them in a format that is ammenable to use by tensorflow."]},{"metadata":{"id":"g10pQhdhbvlu","colab_type":"code","colab":{}},"cell_type":"code","source":["# This loads the training data as a matrix of training examples\n","# and a vector of labels\n","training_data_set = transfer_learning.create_training_dataset(training_images)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NyPJQrITbvlx","colab_type":"text"},"cell_type":"markdown","source":["### Training the classifier"]},{"metadata":{"id":"Qb6bEFDVbvlx","colab_type":"text"},"cell_type":"markdown","source":["We are finally ready to train the new classifier. This is equivalent to replacing & retraining the final layer of our original network with a different number of output classes, so we will still be using tensorflow.\n","\n","#### 9). Complete the following function as instructed in the comments by:\n","* Creating a new dense layer of weights, biases, and logits.\n","* Computing/summarizing the cross-entropy loss.\n","* Creating an optimizer/training step."]},{"metadata":{"id":"G_Iy5EBxbvlz","colab_type":"code","colab":{}},"cell_type":"code","source":["def make_final_layers(bottleneck_tensor, num_classes):\n","    \"\"\"Create the operations for the last layer of the network to be retrained.\n","    \n","    This function should implement a logistic regression layer from the bottleneck\n","    to the labels, trained using gradient descent. We have created the inputs\n","    bottleneck_input and label_input for you. You are responsible for implementing\n","    the logistic regression layer itself (as you have seen in lecture),\n","    the predicted_output (predicted probability for each class), the cross entropy\n","    loss, and the optimization and subsequent training step operation (an\n","    operation that executes one step of gradient descent).  Note: be careful to\n","    use the numerically stable implementation of cross entropy loss as discussed\n","    in lecture.\n","    \n","    You should also record a summary for the loss you compute.\n","    \n","    Parameters\n","    ----------\n","    bottleneck_tensor: the bottleneck tensor in the original network\n","    num_classes: the number of output classes\n","    \n","    Returns\n","    -------\n","    bottleneck_input: the input placeholder for the bottleneck values\n","    label_input: the input placeholder for the label values\n","    logits: the tensor representing the unnormalized log probabilities\n","        for each class.\n","    train_step: an operation representing one gradient descent step.\n","    \"\"\"\n","    bottleneck_tensor_size = int(bottleneck.shape[1])\n","    \n","    with tf.variable_scope('input'):\n","        # This is the input for the bottleneck. It is created\n","        # as a placeholder with default. During training, we will\n","        # be passing in the bottlenecks, but during evaluation,\n","        # the value will be propagated from the bottleneck computed\n","        # from the image using the full network.\n","        bottleneck_input = tf.placeholder_with_default(\n","            bottleneck_tensor,\n","            [None, bottleneck_tensor_size],\n","            'bottleneck_input')\n","        \n","        # This is the input for the label (integer, 1 to number of classes)\n","        label_input = tf.placeholder(tf.int64, [None], name='label_input')\n","    \n","    \n","    # -- Start student must write\n","    # Define weights, biases, and logit transforms\n","    logits = tf.layers.dense(bottleneck_input, num_classes)\n","    \n","    # Compute the cross entropy loss\n","    loss = tf.losses.sparse_softmax_cross_entropy(labels=label_input, logits=logits)\n","    \n","    # Create a summary for the loss\n","    loss_summary = tf.summary.scalar('cross_entropy', loss)\n","    \n","    # Create a Gradient Descent Optimizer\n","    optimizer = tf.train.GradientDescentOptimizer(0.1)\n","    \n","    # Obtain a function which performs a single training step\n","    train_step = optimizer.minimize(loss)\n","    \n","    # -- End student must write\n","    \n","    return bottleneck_input, label_input, logits, train_step, loss_summary"],"execution_count":0,"outputs":[]},{"metadata":{"id":"biW66QUebvl2","colab_type":"text"},"cell_type":"markdown","source":["We will also write a function to evaluate the model's classification accuracy.\n","\n","#### 10). Complete the following function as instructed in the comments."]},{"metadata":{"id":"yjgZmTZ8bvl2","colab_type":"code","colab":{}},"cell_type":"code","source":["def compute_accuracy(labels, logits):\n","    \"\"\"Compute the accuracy for the predicted output.\n","    \n","    Parameters\n","    ----------\n","    labels: The input labels (in a one-hot encoded fashion).\n","    predicted_output: The predicted class probability for each output.\n","    \n","    Returns\n","    -------\n","    A tensor representing the accuracy.\n","    \"\"\"\n","    #-- To be written by student\n","    # Collapse logit output to scalar labels\n","    \n","    # Compute accuracy\n","    with tf.name_scope(\"accuracy\"):\n","        pred = tf.argmax(logits, 1, name=\"pred_class\")\n","        accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, pred), dtype=tf.float32))\n","    # Create summary of accuracy\n","    with tf.variable_scope(\"summary\"):\n","        accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n","    #--\n","    \n","    return accuracy, accuracy_summary"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nMQa4Jicbvl5","colab_type":"text"},"cell_type":"markdown","source":["These functions have been completed for you. Make sure you understand what they are doing as it will help you figure out what to write in the functions above."]},{"metadata":{"id":"njc02SfCbvl7","colab_type":"code","colab":{}},"cell_type":"code","source":["# We create the necessary operations to fine tune the model.\n","\n","with graph.as_default():\n","    bottleneck_input, label_input, logits, train_step, loss_summary = make_final_layers(bottleneck, len(label_maps))\n","    accuracy, accuracy_summary = compute_accuracy(label_input, logits)\n","    summary_op = tf.summary.merge([loss_summary, accuracy_summary])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1rkZ7eS9bvl-","colab_type":"code","colab":{}},"cell_type":"code","source":["def execute_train_step(session: tf.Session, summary_writer: tf.summary.FileWriter, current_step: int):\n","    \"\"\"This function runs a single training step.\n","    \n","    You may wish to print some progress information as you go along.\n","    \n","    Parameters\n","    ----------\n","    session: the tensorflow session to use to run the training step.\n","    summary_writer: the summary file writer to write your summaries to.\n","    current_step: the current step count (starting from zero)\n","    \"\"\"\n","    _, ac, summary = session.run((train_step, accuracy, summary_op),\n","                       feed_dict={bottleneck_input: training_data_set['bottlenecks'],\n","                                  label_input: training_data_set['labels']\n","                                 })\n","    \n","    summary_writer.add_summary(summary, current_step)\n","    \n","    if current_step % 10 == 0:\n","        print('Accuracy at step {0} is {1}'.format(current_step, ac))\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"BvLYrDIDbvmC","colab_type":"code","colab":{}},"cell_type":"code","source":["def evaluate_images(session: tf.Session, images_jpeg_data: [bytes], labels: [int]):\n","    \"\"\"This function will evaluate the accuracy of our model on the specified data.\n","    \n","    Parameters\n","    ----------\n","    session: the tensorflow session to use to run the evaluation step.\n","    images_jpeg_data: a list of strings, with each element in the list corresponding\n","        to the jpeg-encoded data for a given image\n","    labels: a list of integers, with each element in the list corresponding to the label\n","        of a given image.\n","    \n","    Returns\n","    -------\n","    This function should return the accuracy as a floating point number between\n","    0 and 1 (proportion of correctly classified instances).\n","    \"\"\"\n","    correct = []\n","    \n","    for label, jpeg in zip(labels, images_jpeg_data):\n","        image_data = session.run(decoded_image, feed_dict={jpeg_data: jpeg})\n","        ac = session.run(accuracy, feed_dict={resized_input: image_data, label_input: [label]})\n","        correct.append(ac)\n","    \n","    return np.mean(correct)"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"YyVsMqZebvmF","colab_type":"text"},"cell_type":"markdown","source":["Now, we put these functions together to train the final layer. Run the cell and confirm that your functions are working correctly (the accuracy on the training set should be increasing)."]},{"metadata":{"collapsed":true,"id":"Qwut7SwmbvmK","colab_type":"text"},"cell_type":"markdown","source":["#### 11). Navigate to tensorboard and use what you find to answer the follow questions:\n","* What is the accuracy on the test set? How does this compare to random guessing?\n","* How do the training loss and accuracy progress with the number training iterations?\n","* What is the final loss/training error? What might this make you suspicious of? Can you provide a mathematical explanation for why this is happening?\n","* What might you change in order to increase the classification performance of this model?\n","\n","As always, support your with screenshots from tensorboard and references to specific quantitative results."]},{"metadata":{"id":"QajFcM70bNFU","colab_type":"code","colab":{}},"cell_type":"code","source":["# Download and unzip ngrok - you will only need to do this once per session\n","! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip ngrok-stable-linux-amd64.zip\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X_00LwIMbPz0","colab_type":"code","colab":{}},"cell_type":"code","source":["# Launch the ngrok background process\n","get_ipython().system_raw('./ngrok http 6006 &')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CRfz_cWobWps","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get the public URL and be sorted!\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XDmxJSTCbudx","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
