{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of hw4_solution.ipynb","version":"0.3.2","provenance":[{"file_id":"1bN-LLlKCtrlbWHIH2ljYbDnM38ScxoTo","timestamp":1543520684781},{"file_id":"https://github.com/ikinsella/AdvML-Fall-18-Private/blob/master/homework_solutions/hw4/hw4_solution.ipynb","timestamp":1543518917135}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"ViqUvwocbvkv","colab_type":"text"},"cell_type":"markdown","source":["# Homework 4: Transfer learning with the inception network\n","## DUE Friday 17 November at 4pm (both sections)\n","Submit your homework by publishing a notebook that cleanly displays your code, results, and plots to pdf or html\n","\n","## Description\n","\n","In this homework, we will fine-tune the existing [inception network](https://arxiv.org/pdf/1512.00567.pdf) to classify some new categories of images. Building and fitting the network from scratch is expensive and far beyond the scope of this assignment, so you will load the tensorflow graph and variables which have been pre-trained on the imagenet dataset. You are responsible for completing the skeleton code in this notebook in order to use tensorboard to investigate the graph architecture, gain insight into the behavior of the pre-trained network on a new image classification task, fine-tune the network for this new task (retraining the last layer), and finally evaluate the performance of the fine-tuned model.\n","\n","## Downloading the network and data\n","\n","In order to avoid clutter and extraneous technical details, we have created a supplementary module  `transfer_learning.py` that does most of the heavy lifting. We have only exposed the high-level which you are responsible for completing and understanding in full in this notebook.  While you are encouraged to read `transfer_learning.py`, you should not need to modify it in any way.  To import this module into colab, you must first execute the cell below, and then second you must upload the transfer_learning.py file using the upload button that will appear."]},{"metadata":{"id":"ER3YaFuGiim1","colab_type":"code","outputId":"d7d0d2e2-68e3-4b65-c028-6e4671708e36","executionInfo":{"status":"ok","timestamp":1543519095448,"user_tz":300,"elapsed":10174,"user":{"displayName":"Ian August Kinsella","photoUrl":"https://lh6.googleusercontent.com/-NH8dyaeeEH4/AAAAAAAAAAI/AAAAAAAAAAw/T8FW6Z_LULY/s64/photo.jpg","userId":"02721753951188238253"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["# Import transfer_learning module. \n","from google.colab import files\n","src = list(files.upload().values())[0]\n","open('transfer_learning.py','wb').write(src)\n","import transfer_learning"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-b199c244-20b9-41bd-9274-333a0e3e6cd9\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-b199c244-20b9-41bd-9274-333a0e3e6cd9\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving transfer_learning.py to transfer_learning (1).py\n"],"name":"stdout"}]},{"metadata":{"id":"i1SC0xW4bvkx","colab_type":"code","colab":{}},"cell_type":"code","source":["# Import the rest\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import os.path"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hYE1EeBHbvk3","colab_type":"text"},"cell_type":"markdown","source":["We will be using tensorboard to visualize the network, inputs, and other summaries of the training process. After running the following cell, follow the instructions for using tensorboard, evailable in Andrew's excellent tensorflow tutorial."]},{"metadata":{"id":"PuLHDhu7bvk4","colab_type":"code","colab":{}},"cell_type":"code","source":["# Ensure target log dir exists\n","INCEPTION_LOG_DIR = './tmp/inception_v3_log'\n","if not os.path.exists(INCEPTION_LOG_DIR):\n","    os.makedirs(INCEPTION_LOG_DIR)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"USQsTycQbvk7","colab_type":"text"},"cell_type":"markdown","source":["Your goal in the assignment is to use a small collection of photos (500 to be exact) to construct a flower-image classifier capable of discerning differences between daisies, roses, dandelions, sunflowers, and tulips. This presents a obstacle to the use of state-of-the-art image classification models due to the heavily underdetermined nature of the problem (# features = #pixels * #channels >> #images). Moreover, training a large convolutional neural network from the ground up is can be computationally prohibitive. Alternatively, strongly biased linear classifiers like regularized logistic regression avoid these pitfalls. However, the performance of these models will be inadequate, because most important image features are non-linear transformations of the raw input and it is not obvious what these non-linear transformations should be.\n","\n","To circumvent these challenges, we will leverage what a pre-trained convolutional neural network has already learned about important image features from the [imagenet dataset](http://www.image-net.org/) by fine tuning it to our specific problem. To begin, download the the flower dataset [here](http://download.tensorflow.org/example_images/flower_photos.tgz) and extract the contents into a sub-folder of your working directory named ```data```. This dataset is from a [Google Code Lab](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0), though note that we ask you to do considerably more than in that lab, so it is not helpful (nor are the commands covered therein valid answers to this homework).\n","\n","The following cell will partition the images into a training and testing set, then it will load lists of image paths for each."]},{"metadata":{"id":"-gHrqpAFjwuB","colab_type":"code","colab":{}},"cell_type":"code","source":["# download and extract flower data\n","transfer_learning._maybe_download_and_extract('http://download.tensorflow.org/example_images/flower_photos.tgz')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PdbzWbc5bvk8","colab_type":"code","outputId":"dade81ac-f98e-40d8-9161-768a3f94292c","executionInfo":{"status":"ok","timestamp":1543519106978,"user_tz":300,"elapsed":3220,"user":{"displayName":"Ian August Kinsella","photoUrl":"https://lh6.googleusercontent.com/-NH8dyaeeEH4/AAAAAAAAAAI/AAAAAAAAAAw/T8FW6Z_LULY/s64/photo.jpg","userId":"02721753951188238253"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["# For performance reasons, we only use 100 images per class.\n","training_images, testing_images, label_maps = transfer_learning.create_image_lists(\n","    './model/flower_photos', testing_percentage=10, max_number_images=100)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Looking for images in 'tulips'\n","WARNING:tensorflow:WARNING: Folder tulips has more than 100 images. Some images will never be selected.\n","INFO:tensorflow:Looking for images in 'roses'\n","WARNING:tensorflow:WARNING: Folder roses has more than 100 images. Some images will never be selected.\n","INFO:tensorflow:Looking for images in 'daisy'\n","WARNING:tensorflow:WARNING: Folder daisy has more than 100 images. Some images will never be selected.\n","INFO:tensorflow:Looking for images in 'dandelion'\n","WARNING:tensorflow:WARNING: Folder dandelion has more than 100 images. Some images will never be selected.\n","INFO:tensorflow:Looking for images in 'sunflowers'\n","WARNING:tensorflow:WARNING: Folder sunflowers has more than 100 images. Some images will never be selected.\n"],"name":"stdout"}]},{"metadata":{"id":"3IPqEV60bvk_","colab_type":"text"},"cell_type":"markdown","source":["Next, we will load the pre-trained inception model and look at its architecture in tensorboard. \n","\n","#### 1). Complete the following code block by opening a filewriter and pass it the graph object we have created."]},{"metadata":{"id":"bidVvSLXbvk_","colab_type":"code","outputId":"c27b0e77-a396-4201-cc4d-19109c431dcf","executionInfo":{"status":"ok","timestamp":1543519116997,"user_tz":300,"elapsed":11182,"user":{"displayName":"Ian August Kinsella","photoUrl":"https://lh6.googleusercontent.com/-NH8dyaeeEH4/AAAAAAAAAAI/AAAAAAAAAAw/T8FW6Z_LULY/s64/photo.jpg","userId":"02721753951188238253"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"cell_type":"code","source":["# Create the inception model.\n","# Warning! The first time you run this, it will try to download the inception model\n","# from the internet. This is somewhat large (80MB), and can sometimes fail. If\n","# it does, simply try again.\n","\n","graph, bottleneck, resized_input, softmax = transfer_learning.create_model()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /content/transfer_learning.py:97: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.gfile.GFile.\n"],"name":"stdout"}]},{"metadata":{"id":"9tRLMpXGbvlD","colab_type":"code","colab":{}},"cell_type":"code","source":["# Use a summary writer to write the loaded graph and display it in tensorboard.\n","# Then answer the questions below\n","with graph.as_default():\n","    jpeg_data, decoded_image = transfer_learning.make_jpeg_decoding()\n","    #-- Students Complete\n","    summary_writer = tf.summary.FileWriter(os.path.join(INCEPTION_LOG_DIR, 'original'), graph)\n","    summary_writer.close()\n","    #--"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fgsdzYExbvlH","colab_type":"text"},"cell_type":"markdown","source":["#### 2.) In tensorboard, expand the 'softmax' and then the 'logits' block in order to answer to following questions:\n","* What are the dimensions of the inputs/outputs to/from the softmax block?\n","* What are the shape and dimension of the input images?\n","* When considered in isolation, what model is the 'softmax' block implementing? Explain."]},{"metadata":{"id":"dZgFcPPhbvlI","colab_type":"text"},"cell_type":"markdown","source":["Support your answers by including screenshots from tensorboard. (to insert images, simply copy and paste into the notebook)"]},{"metadata":{"id":"REbCNEuabvlJ","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","1.   As seen in the accompanying screenshots, the dimension of the inputs/outputs to the softmax block are 2048 and 1008 respectively. \n","2.   As seen in  the accompanying screenshots, the inputs to the network were 299x299 images with 3 color channels. The total dimension is therefore 268,203.\n","3.   As seen in  the accompanying screenshots, the softmax block is simply a dense layer with a logit activation function. This is equivalent to a multiclass logistic regression model.\n","\n","![alt text](https://drive.google.com/uc?id=1ep3YSe4AITRY2Y0d_AiXpIbZD7HsFu8c)\n","![alt text](https://drive.google.com/uc?id=1vxDzmatw2kCIPfh6Mv7puhCkEVR37TlM)"]},{"metadata":{"id":"e7UOl-tIbvlL","colab_type":"text"},"cell_type":"markdown","source":["## Classifying out of the box\n","\n","Before retraining or modifying the network, we would like to better understand how it behaves on our dataset out of the box. Therefore, we will use it classify a couple of our flower images. First, we must create summary operations to write quanities of interest to tensorboard so that we may investigate. \n","\n","#### 3.) Complete the block below by implementing the following summaries:\n","* `output_summary`: a summary of output probabilities (histogram of the output of the softmax block)\n","* `input_summary`: a summary of image input to the network (image of resized and decoded jpeg data)\n","* `bottleneck_summary`: a summary of bottleneck activations (histogram of the input to the softmax block)"]},{"metadata":{"id":"MafS0VfabvlM","colab_type":"code","colab":{}},"cell_type":"code","source":["with graph.as_default():\n","    # Define summaries for tensorboard\n","    #-- Students complete\n","    output_summary = tf.summary.histogram('softmax_output', softmax)  # histogram of softmax output\n","    input_summary = tf.summary.image('input_image', resized_input)  # display image of input\n","    bottleneck_summary = tf.summary.histogram('bottleneck_activations', bottleneck)  # second to last layer output\n","    #-- \n","    summary_op = tf.summary.merge_all()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"co28sR9mbvlT","colab_type":"text"},"cell_type":"markdown","source":["Next, we will need a function that streamlines the process of performing inference given an image. This function will take as input the tensorflow session, a path to the image, and the ```summary_op``` we have created above. It will then open a ```t.summary.FileWriter``` and run the network forwards to obtain the formatted image, softmax output, and summaries. \n","\n","#### 4). Complete the code block below as instructed in each of the comments."]},{"metadata":{"id":"cRHlywDgbvlV","colab_type":"code","colab":{}},"cell_type":"code","source":["def classify_image(session, image_path, summary_op):\n","    \"\"\"This functions reads a single image from disk and classifies\n","    the image using the pre-trained network.\n","    \n","    Parameters\n","    ----------\n","    session: the tensorflow session to use to run the operation\n","    image_path: a string corresponding to the path of the image to load\n","    summary_op: the summary operation.\n","    \n","    Returns\n","    -------\n","    label: an integer representing the label of the classified example.\n","    softmax_output: the network's output multinomial probabilities\n","    \"\"\"\n","    \n","    # Open single image file and extract data\n","    with open(image_path, 'rb') as f:\n","        image_data = f.read()\n","    \n","    #-- Students Complete\n","    # Create Summary writer object\n","    summary_writer = tf.summary.FileWriter(os.path.join(INCEPTION_LOG_DIR, 'original'), graph)\n","        \n","    # Run pre-defined pipeline to decode & resize raw image data into decoded_image \n","    image = session.run(decoded_image, feed_dict={jpeg_data: image_data})\n","        \n","    # Run input through network and obtain output from last layer, defined by create_model() call above.\n","    # Note: be sure to run your summary operation as well.\n","    softmax_output, summary = session.run([softmax, summary_op], feed_dict={resized_input: image})\n","    \n","    # Log summary & terminate writer in the usual way.\n","    summary_writer.add_summary(summary)\n","    summary_writer.close()\n","    #--\n","    \n","    # Return label\n","    return(np.argmax(softmax_output),softmax_output)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4ydvij6BbvlY","colab_type":"text"},"cell_type":"markdown","source":["We are now ready to classify some images. Run the code in this cell in order to obtain the output label and navigate to tensorboard to look at the histogram and image summaries that have been logged. **Note:** You may need to refresh Tensorboard in order for this to display."]},{"metadata":{"id":"O1jHmDjhbvlZ","colab_type":"code","outputId":"22a2cc3c-b9e2-42f2-b04e-26d51493efc0","executionInfo":{"status":"ok","timestamp":1543519137628,"user_tz":300,"elapsed":1936,"user":{"displayName":"Ian August Kinsella","photoUrl":"https://lh6.googleusercontent.com/-NH8dyaeeEH4/AAAAAAAAAAI/AAAAAAAAAAw/T8FW6Z_LULY/s64/photo.jpg","userId":"02721753951188238253"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["image_path = os.path.join(os.getcwd(), 'model', 'flower_photos', 'daisy', '100080576_f52e8ee070_n.jpg')\n","with graph.as_default():\n","    with tf.Session() as session:\n","        # We classify the image and print the label\n","        imagenet_label,_ = classify_image(session, image_path, summary_op)\n","print(imagenet_label)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["357\n"],"name":"stdout"}]},{"metadata":{"id":"yDqWI37xbvlc","colab_type":"text"},"cell_type":"markdown","source":["#### 5.) Confirm that your code works by taking a screenshot of the input image in tensorboard."]},{"metadata":{"id":"jmVSM3IFbvld","colab_type":"text"},"cell_type":"markdown","source":["![alt text](https://drive.google.com/uc?id=158LtEL2D8Otf2KeS7uf4XqIHakfK666Q)"]},{"metadata":{"id":"9cxUhc-_bvld","colab_type":"text"},"cell_type":"markdown","source":["If you've completed the prevous steps correctly, you should see that the pre-trained network assigns the label 357 to our image. Cross referencing this with a [source](https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57) that explains what each of the imagenet labels are, we see that our classifier is right on target. The image was indeed a daisy, but this is solely a consequence of the fact that the class 'daisy' is included in the imagenet dataset. We should not expect such favorable outcome when this is not the case. We will test this assumtion by classifying a different type of flower image."]},{"metadata":{"id":"7aaVgzzjbvle","colab_type":"code","outputId":"3850dbfa-34f0-4cca-f907-a0940de6bf22","executionInfo":{"status":"ok","timestamp":1543519169866,"user_tz":300,"elapsed":1838,"user":{"displayName":"Ian August Kinsella","photoUrl":"https://lh6.googleusercontent.com/-NH8dyaeeEH4/AAAAAAAAAAI/AAAAAAAAAAw/T8FW6Z_LULY/s64/photo.jpg","userId":"02721753951188238253"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["image_path = os.path.join(os.getcwd(), 'model', 'flower_photos', 'roses', '19440805164_920b28da61_n.jpg')\n","with graph.as_default():\n","    with tf.Session() as session:\n","        imagenet_label,softmax_output = classify_image(session, image_path, summary_op)\n","print(imagenet_label)\n","print(softmax_output[0,imagenet_label])"],"execution_count":16,"outputs":[{"output_type":"stream","text":["874\n","0.5218311\n"],"name":"stdout"}]},{"metadata":{"id":"66sFKYUcbvlg","colab_type":"text"},"cell_type":"markdown","source":["#### 6.) Please explain what you have found by answering the following questions:\n","\n","* What does the network think this is an image of? Is this reasonable?\n","* How confident is it in assigning this label (what is the probability it assigns)?\n","* Why might the network behave this way?\n","* What warning might you give about deep-learning/machine-learning in general based on these observations?\n","\n","Support your answers by referencing screenshots from tensorboard (of the input picture, softmax output, etc.)."]},{"metadata":{"id":"fLNr9imUbvlj","colab_type":"text"},"cell_type":"markdown","source":["1.   The network has classified the picture of a rose as a vase. This is surprising at first.\n","2.   It assigns a probability of .52 to the classification which is rather strong given the large number of classes. (This can be seen as compared to other classes in the histogram tab of tensorboard below)\n","3.   It is possible that in the training dataset many images of vases were accompanied by flowers (as it is common to keep flowers in vases). If this were the case the network could have associated image features associated with these flowers (many of which many have been roses) which in turn would have associated rose-like image features with the vase classficiation label.\n","4.   When training highly flexible models, the ability to generalize depends heavily on the size and diversity of your training dataset.\n","\n","![alt text](https://drive.google.com/uc?id=1-oMIWcTHQEhBBJNl6Hdc25B_3duTQs8A)"]},{"metadata":{"id":"-txuVP3Abvlk","colab_type":"text"},"cell_type":"markdown","source":["## Adapting The Network\n","\n","Clearly, this model will not suffice for our application out of the box. However, it is not unreasonable to think that we may be able to adapt what this network has \"learned\" about processing images for our purposes. Remember that the last layer of the network implements a well known classifier (see your answer to question 2). Under this interpretation, the layers of the network up until the last are responsible for learning a non-linear mapping between raw inputs (pixel values) and useful derived features. Consequently, we may remove the last layer, fix the weights of the remaining layers, use what remains as a black-box function transforming images into derived feature vectors, and finally fit a new classifier on the derived feature vectors. "]},{"metadata":{"id":"OAsNpY_Nbvll","colab_type":"text"},"cell_type":"markdown","source":["### Pre-Computing the bottlenecks\n","\n","To train the new classifier efficiently, we will want to pre-compute the derived feature vectors. We have been calling these the 'bottleneck', because they would limit the speed of training if we had to be re-computed them iteration. Additionally, its small size relative to the dimension of our raw input is essential in ensures that it provides a compact summary of the images for our classifier.\n","\n","#### 7). Which block in the tensorboard graph is the bottleneck?"]},{"metadata":{"id":"-Co-3jD6bvln","colab_type":"text"},"cell_type":"markdown","source":["As seen in the accompanying screenshot, the output 'pool_3' is what we refer to as the 'bottlenecks'.\n","\n","![alt text](https://drive.google.com/uc?id=1l6FPc45E20c2SIMS6s1K7bJdVpFhoG6I)"]},{"metadata":{"id":"W5KACbaabvln","colab_type":"text"},"cell_type":"markdown","source":["We have written code that will cache the derived features or 'bottleneck activation' for each images. In order to use this, we require that you complete a simple function that returns these activations.\n","\n","#### 8). Complete the code block below and compute the bottlenecks."]},{"metadata":{"id":"WXSuTYZObvlp","colab_type":"code","colab":{}},"cell_type":"code","source":["def compute_bottleneck(session, image_data):\n","    \"\"\"Computes the bottleneck for a given image\n","    \n","    Parameters\n","    ----------\n","    session: the tensorflow session to use for the computation.\n","    jpeg_data_tensor: the tensor to feed the jpeg data into.\n","    bottleneck_tensor: the tensor representing.\n","    image_data: a byte sequence representing the encoded jpeg data.\n","    \n","    Returns\n","    -------\n","    A numpy array containing the bottleneck information for the image.\n","    \"\"\"\n","    #-- To be completed by the student\n","    image = session.run(decoded_image, feed_dict={jpeg_data: image_data})\n","    return session.run(bottleneck, feed_dict={resized_input: image})\n","    #--"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O0om-VUGbvlr","colab_type":"code","outputId":"9beb2515-71d1-4105-9bb4-50b14053d1aa","executionInfo":{"status":"ok","timestamp":1543519192170,"user_tz":300,"elapsed":13136,"user":{"displayName":"Ian August Kinsella","photoUrl":"https://lh6.googleusercontent.com/-NH8dyaeeEH4/AAAAAAAAAAI/AAAAAAAAAAw/T8FW6Z_LULY/s64/photo.jpg","userId":"02721753951188238253"}},"colab":{"base_uri":"https://localhost:8080/","height":425}},"cell_type":"code","source":["# This cell generates all the bottlenecks. Warning: it may take a while\n","# The results are cached so you only need to do it once -- if you change\n","# your compute_bottleneck function, you will need to delete the existing\n","# files to force the notebook to regenerate the bottlenecks (they are\n","# found in ./data/bottlenecks)\n","\n","with graph.as_default():\n","    with tf.Session() as session:\n","        transfer_learning.cache_bottlenecks(compute_bottleneck, session, training_images)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Saved 1/444 bottlenecks\n","Saved 21/444 bottlenecks\n","Saved 41/444 bottlenecks\n","Saved 61/444 bottlenecks\n","Saved 81/444 bottlenecks\n","Saved 101/444 bottlenecks\n","Saved 121/444 bottlenecks\n","Saved 141/444 bottlenecks\n","Saved 161/444 bottlenecks\n","Saved 181/444 bottlenecks\n","Saved 201/444 bottlenecks\n","Saved 221/444 bottlenecks\n","Saved 241/444 bottlenecks\n","Saved 261/444 bottlenecks\n","Saved 281/444 bottlenecks\n","Saved 301/444 bottlenecks\n","Saved 321/444 bottlenecks\n","Saved 341/444 bottlenecks\n","Saved 361/444 bottlenecks\n","Saved 381/444 bottlenecks\n","Saved 401/444 bottlenecks\n","Saved 421/444 bottlenecks\n","Saved 441/444 bottlenecks\n","Done computing bottlenecks!\n"],"name":"stdout"}]},{"metadata":{"id":"nE-IaGa6bvlu","colab_type":"text"},"cell_type":"markdown","source":["Now that these have been saved, we can load them in a format that is ammenable to use by tensorflow."]},{"metadata":{"id":"g10pQhdhbvlu","colab_type":"code","colab":{}},"cell_type":"code","source":["# This loads the training data as a matrix of training examples\n","# and a vector of labels\n","training_data_set = transfer_learning.create_training_dataset(training_images)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NyPJQrITbvlx","colab_type":"text"},"cell_type":"markdown","source":["### Training the classifier"]},{"metadata":{"id":"Qb6bEFDVbvlx","colab_type":"text"},"cell_type":"markdown","source":["We are finally ready to train the new classifier. This is equivalent to replacing & retraining the final layer of our original network with a different number of output classes, so we will still be using tensorflow.\n","\n","#### 9). Complete the following function as instructed in the comments by:\n","* Creating a new dense layer of weights, biases, and logits.\n","* Computing/summarizing the cross-entropy loss.\n","* Creating an optimizer/training step."]},{"metadata":{"id":"G_Iy5EBxbvlz","colab_type":"code","colab":{}},"cell_type":"code","source":["def make_final_layers(bottleneck_tensor, num_classes):\n","    \"\"\"Create the operations for the last layer of the network to be retrained.\n","    \n","    This function should implement a logistic regression layer from the bottleneck\n","    to the labels, trained using gradient descent. We have created the inputs\n","    bottleneck_input and label_input for you. You are responsible for implementing\n","    the logistic regression layer itself (as you have seen in lecture),\n","    the predicted_output (predicted probability for each class), the cross entropy\n","    loss, and the optimization and subsequent training step operation (an\n","    operation that executes one step of gradient descent).  Note: be careful to\n","    use the numerically stable implementation of cross entropy loss as discussed\n","    in lecture.\n","    \n","    You should also record a summary for the loss you compute.\n","    \n","    Parameters\n","    ----------\n","    bottleneck_tensor: the bottleneck tensor in the original network\n","    num_classes: the number of output classes\n","    \n","    Returns\n","    -------\n","    bottleneck_input: the input placeholder for the bottleneck values\n","    label_input: the input placeholder for the label values\n","    logits: the tensor representing the unnormalized log probabilities\n","        for each class.\n","    train_step: an operation representing one gradient descent step.\n","    \"\"\"\n","    bottleneck_tensor_size = int(bottleneck.shape[1])\n","    \n","    with tf.variable_scope('input'):\n","        # This is the input for the bottleneck. It is created\n","        # as a placeholder with default. During training, we will\n","        # be passing in the bottlenecks, but during evaluation,\n","        # the value will be propagated from the bottleneck computed\n","        # from the image using the full network.\n","        bottleneck_input = tf.placeholder_with_default(\n","            bottleneck_tensor,\n","            [None, bottleneck_tensor_size],\n","            'bottleneck_input')\n","        \n","        # This is the input for the label (integer, 1 to number of classes)\n","        label_input = tf.placeholder(tf.int64, [None], name='label_input')\n","    \n","    \n","    # -- Start student must write\n","    # Define weights, biases, and logit transforms\n","    logits = tf.layers.dense(bottleneck_input, num_classes)\n","    # Compute the cross entropy loss\n","    loss = tf.losses.sparse_softmax_cross_entropy(labels=label_input, logits=logits)\n","    # Create a summary for the loss\n","    loss_summary = tf.summary.scalar('cross_entropy', loss)\n","    # Create a Gradient Descent Optimizer\n","    optimizer = tf.train.GradientDescentOptimizer(0.1)\n","    # Obtain a function which performs a single training step\n","    train_step = optimizer.minimize(loss)\n","    # -- End student must write\n","    \n","    return bottleneck_input, label_input, logits, train_step, loss_summary"],"execution_count":0,"outputs":[]},{"metadata":{"id":"biW66QUebvl2","colab_type":"text"},"cell_type":"markdown","source":["We will also write a function to evaluate the model's classification accuracy.\n","\n","#### 10). Complete the following function as instructed in the comments."]},{"metadata":{"id":"yjgZmTZ8bvl2","colab_type":"code","colab":{}},"cell_type":"code","source":["def compute_accuracy(labels, logits):\n","    \"\"\"Compute the accuracy for the predicted output.\n","    \n","    Parameters\n","    ----------\n","    labels: The input labels (in a one-hot encoded fashion).\n","    predicted_output: The predicted class probability for each output.\n","    \n","    Returns\n","    -------\n","    A tensor representing the accuracy.\n","    \"\"\"\n","    #-- To be written by student\n","    # Collapse logit output to scalar labels\n","    predicted_label = tf.argmax(logits, 1)\n","    # Compute accuracy\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_label, labels), tf.float32))\n","    # create summary of accuracy\n","    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n","    #--\n","    \n","    return accuracy, accuracy_summary"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nMQa4Jicbvl5","colab_type":"text"},"cell_type":"markdown","source":["These functions have been completed for you. Make sure you understand what they are doing as it will help you figure out what to write in the functions above."]},{"metadata":{"id":"njc02SfCbvl7","colab_type":"code","colab":{}},"cell_type":"code","source":["# We create the necessary operations to fine tune the model.\n","\n","with graph.as_default():\n","    bottleneck_input, label_input, logits, train_step, loss_summary = make_final_layers(bottleneck, len(label_maps))\n","    accuracy, accuracy_summary = compute_accuracy(label_input, logits)\n","    summary_op = tf.summary.merge([loss_summary, accuracy_summary])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1rkZ7eS9bvl-","colab_type":"code","colab":{}},"cell_type":"code","source":["def execute_train_step(session: tf.Session, summary_writer: tf.summary.FileWriter, current_step: int):\n","    \"\"\"This function runs a single training step.\n","    \n","    You may wish to print some progress information as you go along.\n","    \n","    Parameters\n","    ----------\n","    session: the tensorflow session to use to run the training step.\n","    summary_writer: the summary file writer to write your summaries to.\n","    current_step: the current step count (starting from zero)\n","    \"\"\"\n","    _, ac, summary = session.run((train_step, accuracy, summary_op),\n","                       feed_dict={bottleneck_input: training_data_set['bottlenecks'],\n","                                  label_input: training_data_set['labels']\n","                                 })\n","    \n","    summary_writer.add_summary(summary, current_step)\n","    \n","    if current_step % 10 == 0:\n","        print('Accuracy at step {0} is {1}'.format(current_step, ac))\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"BvLYrDIDbvmC","colab_type":"code","colab":{}},"cell_type":"code","source":["def evaluate_images(session: tf.Session, images_jpeg_data: [bytes], labels: [int]):\n","    \"\"\"This function will evaluate the accuracy of our model on the specified data.\n","    \n","    Parameters\n","    ----------\n","    session: the tensorflow session to use to run the evaluation step.\n","    images_jpeg_data: a list of strings, with each element in the list corresponding\n","        to the jpeg-encoded data for a given image\n","    labels: a list of integers, with each element in the list corresponding to the label\n","        of a given image.\n","    \n","    Returns\n","    -------\n","    This function should return the accuracy as a floating point number between\n","    0 and 1 (proportion of correctly classified instances).\n","    \"\"\"\n","    correct = []\n","    \n","    for label, jpeg in zip(labels, images_jpeg_data):\n","        image_data = session.run(decoded_image, feed_dict={jpeg_data: jpeg})\n","        ac = session.run(accuracy, feed_dict={resized_input: image_data, label_input: [label]})\n","        correct.append(ac)\n","    \n","    return np.mean(correct)"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"YyVsMqZebvmF","colab_type":"text"},"cell_type":"markdown","source":["Now, we put these functions together to train the final layer. Run the cell and confirm that your functions are working correctly (the accuracy on the training set should be increasing)."]},{"metadata":{"id":"8fH_WKcGbvmG","colab_type":"code","outputId":"5b9fe56b-ef91-4907-ba29-40d470e2f9b6","executionInfo":{"status":"ok","timestamp":1543519211584,"user_tz":300,"elapsed":22336,"user":{"displayName":"Ian August Kinsella","photoUrl":"https://lh6.googleusercontent.com/-NH8dyaeeEH4/AAAAAAAAAAI/AAAAAAAAAAw/T8FW6Z_LULY/s64/photo.jpg","userId":"02721753951188238253"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"cell_type":"code","source":["# Let's run the training and evaluation!\n","\n","with graph.as_default():\n","    with tf.Session() as session:\n","        print('------------- Starting training ----------------')\n","        session.run(tf.global_variables_initializer())\n","        summary_writer = tf.summary.FileWriter(os.path.join(INCEPTION_LOG_DIR, 'retrained'), graph)\n","        for i in range(100):\n","            execute_train_step(session, summary_writer, i)\n","        summary_writer.close()  \n","        print('------------- Training done! -------------------')\n","        print('---------- Loading testing data ----------------')\n","        tlabels, timages = transfer_learning.get_testing_data(testing_images)\n","        print('----------- Evaluating on testing --------------')\n","        \n","        eval_accuracy = evaluate_images(session, timages, tlabels)\n","        print('Evaluation accuracy was: {0}'.format(eval_accuracy))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["------------- Starting training ----------------\n","Accuracy at step 0 is 0.20495495200157166\n","Accuracy at step 10 is 0.542792797088623\n","Accuracy at step 20 is 0.7905405163764954\n","Accuracy at step 30 is 0.8423423171043396\n","Accuracy at step 40 is 0.954954981803894\n","Accuracy at step 50 is 0.9729729890823364\n","Accuracy at step 60 is 0.9842342138290405\n","Accuracy at step 70 is 0.9887387156486511\n","Accuracy at step 80 is 0.9909909963607788\n","Accuracy at step 90 is 0.9954954981803894\n","------------- Training done! -------------------\n","---------- Loading testing data ----------------\n","----------- Evaluating on testing --------------\n","Evaluation accuracy was: 0.8571428656578064\n"],"name":"stdout"}]},{"metadata":{"collapsed":true,"id":"Qwut7SwmbvmK","colab_type":"text"},"cell_type":"markdown","source":["#### 11). Navigate to tensorboard and use what you find to answer the follow questions:\n","* What is the accuracy on the test set? How does this compare to random guessing?\n","* How do the training loss and accuracy progress with the number training iterations?\n","* What is the final loss/training error? What might this make you suspicious of? Can you provide a mathematical explanation for why this is happening?\n","* What might you change in order to increase the classification performance of this model?\n","\n","As always, support your with screenshots from tensorboard and references to specific quantitative results."]},{"metadata":{"id":"qL_fMjUr6cw2","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","1.   The accuracy on the test set is approximately 85%, this is significantly better than the 20% we could expect to achieve from random guessing.\n","2.   As the iterations progress, both quantities quickly approach 0 and 100% respectively at which point they plateua and approach these quantities more gradually (see screenshots below).\n","3.   Since the final loss and misclassification rates approach 0, we should immediately suspect overfitting. In fact, we know this will occur since the feature space for our unregularized logistic regression model is 2048 yet we only have ~500 training data points. Therefore, we know there are infinitely many inear decision boundaries which perfectly separate our data. As discussed in class and in previous homeworks, logistic regression classifiers are prone to overfitting in these scenarios.\n","4.   One possible solution would be to add regularization to the weights of the logistic regression classifier.\n","\n","![alt text](https://drive.google.com/uc?id=1iYJD4MLNE_umHWkDseVF_9h3-aHDsjWV)\n","\n"]},{"metadata":{"id":"p59YdOulbvmM","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}